{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP modeling\n",
    "Logistic feature selection\n",
    "Optuna\n",
    "Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'C:\\Users\\leeya\\OneDrive\\Desktop\\부캠 수업자료\\Final\\train_common_v3.3_피처삭제X_군집_1111.csv')\n",
    "test = pd.read_csv(r'C:\\Users\\leeya\\OneDrive\\Desktop\\부캠 수업자료\\Final\\test_common_v3.3_피처삭제X_군집_1111.csv')\n",
    "target =  pd.read_csv(r'C:\\Users\\leeya\\OneDrive\\Desktop\\부캠 수업자료\\Final\\store_train.csv')\n",
    "submit = pd.read_csv(r'C:\\Users\\leeya\\OneDrive\\Desktop\\부캠 수업자료\\Final\\store_submission (2).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14940, 1471), (14940, 2), (12225, 1471), (12225, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape , target.shape , test.shape , submit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum().sum(), test.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14940, 1470), (12225, 1470))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ft = train.iloc[:,1:]\n",
    "test_ft = test.iloc[:,1:]\n",
    "train_ft.shape, test_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "주구매지점        4\n",
       "주구매_대분류     22\n",
       "주구매_중분류    238\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = train_ft.select_dtypes(\"object\").columns.tolist()\n",
    "train_ft[cols].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cols = train_ft.select_dtypes(include=['object']).columns\n",
    "enc = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "enc.fit(train_ft[cols])\n",
    "\n",
    "tmp = pd.DataFrame(\n",
    "    enc.transform(train_ft[cols]).toarray(),\n",
    "    columns = enc.get_feature_names_out()\n",
    ")\n",
    "train_ft = pd.concat([train_ft, tmp], axis=1).drop(columns=cols)\n",
    "\n",
    "tmp = pd.DataFrame(\n",
    "    enc.transform(test_ft[cols]).toarray(),\n",
    "    columns = enc.get_feature_names_out()\n",
    ")\n",
    "test_ft = pd.concat([test_ft, tmp], axis=1).drop(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index([], dtype='object'), Index([], dtype='object'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ft.select_dtypes(\"object\").columns , test_ft.select_dtypes(\"object\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>내점일수</th>\n",
       "      <th>구매주기</th>\n",
       "      <th>주말방문비율</th>\n",
       "      <th>평일방문비율</th>\n",
       "      <th>주말방문횟수</th>\n",
       "      <th>평일방문횟수</th>\n",
       "      <th>봄_구매비율</th>\n",
       "      <th>여름_구매비율</th>\n",
       "      <th>가을_구매비율</th>\n",
       "      <th>겨울_구매비율</th>\n",
       "      <th>...</th>\n",
       "      <th>주구매_중분류_핸드백행사</th>\n",
       "      <th>주구매_중분류_행사</th>\n",
       "      <th>주구매_중분류_행사슈즈</th>\n",
       "      <th>주구매_중분류_행사핸드백</th>\n",
       "      <th>주구매_중분류_향수</th>\n",
       "      <th>주구매_중분류_헤어ACC</th>\n",
       "      <th>주구매_중분류_홈데코</th>\n",
       "      <th>주구매_중분류_화장잡화</th>\n",
       "      <th>주구매_중분류_화장품</th>\n",
       "      <th>주구매_중분류_훼미닌부틱</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.041494</td>\n",
       "      <td>0.130682</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.029014</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.082988</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.079304</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.228216</td>\n",
       "      <td>0.034091</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.154839</td>\n",
       "      <td>0.174081</td>\n",
       "      <td>0.464912</td>\n",
       "      <td>0.140351</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.219298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.377593</td>\n",
       "      <td>0.017045</td>\n",
       "      <td>0.189573</td>\n",
       "      <td>0.810427</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.330754</td>\n",
       "      <td>0.379147</td>\n",
       "      <td>0.180095</td>\n",
       "      <td>0.236967</td>\n",
       "      <td>0.203791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.107884</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.103226</td>\n",
       "      <td>0.088975</td>\n",
       "      <td>0.112903</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.209677</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1731 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       내점일수      구매주기    주말방문비율    평일방문비율    주말방문횟수    평일방문횟수    봄_구매비율  \\\n",
       "0  0.041494  0.130682  0.250000  0.750000  0.032258  0.029014  0.050000   \n",
       "1  0.082988  0.090909  0.023810  0.976190  0.006452  0.079304  0.357143   \n",
       "2  0.228216  0.034091  0.210526  0.789474  0.154839  0.174081  0.464912   \n",
       "3  0.377593  0.017045  0.189573  0.810427  0.258065  0.330754  0.379147   \n",
       "4  0.107884  0.062500  0.258065  0.741935  0.103226  0.088975  0.112903   \n",
       "\n",
       "    여름_구매비율   가을_구매비율   겨울_구매비율  ...  주구매_중분류_핸드백행사  주구매_중분류_행사  주구매_중분류_행사슈즈  \\\n",
       "0  0.250000  0.400000  0.300000  ...            0.0         0.0           0.0   \n",
       "1  0.166667  0.357143  0.119048  ...            0.0         0.0           0.0   \n",
       "2  0.140351  0.175439  0.219298  ...            0.0         0.0           0.0   \n",
       "3  0.180095  0.236967  0.203791  ...            0.0         0.0           0.0   \n",
       "4  0.612903  0.209677  0.064516  ...            0.0         0.0           0.0   \n",
       "\n",
       "   주구매_중분류_행사핸드백  주구매_중분류_향수  주구매_중분류_헤어ACC  주구매_중분류_홈데코  주구매_중분류_화장잡화  \\\n",
       "0            0.0         0.0            0.0          0.0           0.0   \n",
       "1            0.0         0.0            0.0          0.0           0.0   \n",
       "2            0.0         0.0            0.0          0.0           0.0   \n",
       "3            0.0         0.0            0.0          0.0           0.0   \n",
       "4            0.0         0.0            0.0          0.0           0.0   \n",
       "\n",
       "   주구매_중분류_화장품  주구매_중분류_훼미닌부틱  \n",
       "0          0.0            0.0  \n",
       "1          0.0            0.0  \n",
       "2          0.0            0.0  \n",
       "3          0.0            0.0  \n",
       "4          0.0            0.0  \n",
       "\n",
       "[5 rows x 1731 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_ft[train_ft.columns] = scaler.fit_transform(train_ft)\n",
    "test_ft[test_ft.columns] = scaler.fit_transform(test_ft)\n",
    "train_ft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1.0\n",
       "1        1.0\n",
       "2        0.0\n",
       "3        0.0\n",
       "4        0.0\n",
       "        ... \n",
       "14935    0.0\n",
       "14936    0.0\n",
       "14937    0.0\n",
       "14938    1.0\n",
       "14939    1.0\n",
       "Name: target, Length: 14940, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = target[\"target\"]\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_ft, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\leeya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\leeya\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\leeya\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (8.26.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\leeya\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\leeya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\leeya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in c:\\users\\leeya\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\leeya\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\leeya\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\leeya\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\leeya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\leeya\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\leeya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\leeya\\appdata\\roaming\\python\\python312\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\leeya\\appdata\\roaming\\python\\python312\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\leeya\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\leeya\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\leeya\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\leeya\\appdata\\roaming\\python\\python312\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: 691\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Logistic 피처셀렉션\n",
    "logistic = LogisticRegression(penalty=\"l2\", C=1.0, solver=\"liblinear\", random_state=SEED)\n",
    "selector = SelectFromModel(logistic)\n",
    "selected_features = selector.fit_transform(train_ft, target)\n",
    "\n",
    "\n",
    "selected_cols = selector.get_feature_names_out()\n",
    "print(f\"Selected features: {len(selected_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# # Optuna 하이퍼파라미터 튜닝\n",
    "# def objective(trial):\n",
    "#     # MLPClassifier 하이퍼파라미터 설정\n",
    "#     n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "#     hidden_layer_sizes = tuple([trial.suggest_int(f\"n_units_l{i}\", 10, 100) for i in range(n_layers)])\n",
    "\n",
    "#     hp = {\n",
    "#         \"hidden_layer_sizes\": hidden_layer_sizes,\n",
    "#         \"activation\": trial.suggest_categorical(\"activation\", [\"relu\", \"tanh\"]),\n",
    "#         \"solver\": trial.suggest_categorical(\"solver\", [\"adam\", \"sgd\"]),\n",
    "#         \"alpha\": trial.suggest_float(\"alpha\", 1e-5, 1e-1, log=True),\n",
    "#         \"learning_rate_init\": trial.suggest_float(\"learning_rate_init\", 1e-5, 1e-1, log=True),\n",
    "#         \"max_iter\": trial.suggest_int(\"max_iter\", 10, 200, step=10),\n",
    "#         \"early_stopping\": True,\n",
    "#         \"validation_fraction\": trial.suggest_float(\"validation_fraction\", 0.1, 0.3, step=0.05),\n",
    "#         \"n_iter_no_change\": trial.suggest_int(\"n_iter_no_change\", 5, 15),\n",
    "#     }\n",
    "\n",
    "\n",
    "#     model = MLPClassifier(**hp, random_state=SEED)\n",
    "\n",
    "\n",
    "#     score = cross_val_score(model, selected_features, target, cv=cv, scoring=\"f1_macro\", n_jobs=-1).mean()\n",
    "\n",
    "#     return score\n",
    "\n",
    "\n",
    "# sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "# study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "# study.optimize(objective, n_trials=100)\n",
    "\n",
    "\n",
    "# print(\"Best hyperparameters:\", study.best_params)\n",
    "# print(\"Best F1 Score:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optuna 튜닝 결과\n",
    "# best_mlp_params = {\n",
    "#     'hidden_layer_sizes': (88, 89), \n",
    "#     'activation': 'relu',\n",
    "#     'solver': 'adam',\n",
    "#     'alpha': 0.001230314130334757,\n",
    "#     'learning_rate_init': 0.0016991055449377132,\n",
    "#     'max_iter': 500,\n",
    "#     'validation_fraction': 0.15000000000000002,\n",
    "#     'n_iter_no_change': 10,\n",
    "#     'early_stopping': True\n",
    "# }\n",
    "\n",
    "\n",
    "# scores = []\n",
    "# models = []\n",
    "\n",
    "# for tri, vai in cv.split(train_ft, target):\n",
    "#     x_train = train_ft.iloc[tri][selected_cols] #logistic 으로 선택된 피처만 사용가능\n",
    "#     y_train = target.iloc[tri]\n",
    "#     x_valid = train_ft.iloc[vai][selected_cols]  \n",
    "#     y_valid = target.iloc[vai]\n",
    "\n",
    "#     mlp_model = MLPClassifier(**best_mlp_params, random_state=SEED)\n",
    "#     mlp_model.fit(x_train, y_train)\n",
    "\n",
    "#     models.append(mlp_model)\n",
    "\n",
    "#     pred = mlp_model.predict(x_valid)\n",
    "#     score = f1_score(y_valid, pred, average='macro')\n",
    "#     scores.append(score)\n",
    "\n",
    "# #f1 출력\n",
    "# print(\"Mean F1 Macro Score:\", np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 Macro Score (Stacking Ensemble): 0.727301750399499\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# 최적화된 MLP 파라미터\n",
    "best_mlp_params = {\n",
    "    'hidden_layer_sizes': (88, 89),\n",
    "    'activation': 'relu',\n",
    "    'solver': 'adam',\n",
    "    'alpha': 0.001230314130334757,\n",
    "    'learning_rate_init': 0.0016991055449377132,\n",
    "    'max_iter': 500,\n",
    "    'validation_fraction': 0.15,\n",
    "    'n_iter_no_change': 10,\n",
    "    'early_stopping': True\n",
    "}\n",
    "\n",
    "best_xgb_params = {\n",
    "    'n_estimators': 900,\n",
    "    'learning_rate': 0.021734683976721573,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 5,\n",
    "    'subsample': 0.7039147285447092,\n",
    "    'colsample_bytree': 0.5823954097077503,\n",
    "    'gamma': 2.3107011128410493,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "best_lgbm_params = {\n",
    "    'num_leaves': 33,\n",
    "    'max_depth': 11,\n",
    "    'learning_rate': 0.01944354088862471,\n",
    "    'n_estimators': 800,\n",
    "    'min_child_samples': 65,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 3.6529269182968447e-07,\n",
    "    'reg_lambda': 3.3190277841665685e-07,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "\n",
    "# Stacking Base Estimators 설정\n",
    "estimators = [\n",
    "    ('mlp', MLPClassifier(**best_mlp_params, random_state=SEED)),\n",
    "    #('rf', RandomForestClassifier(random_state=SEED)),\n",
    "    ('xgb', XGBClassifier(random_state=SEED)),\n",
    "    ('lgbm', LGBMClassifier(random_state=SEED))\n",
    "]\n",
    "\n",
    "# Final Estimator 설정\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 교차 검증\n",
    "scores = []\n",
    "models = []\n",
    "\n",
    "# 교차 검증 루프\n",
    "for tri, vai in cv.split(train_ft, target):\n",
    "    x_train = train_ft.iloc[tri][selected_cols]\n",
    "    y_train = target.iloc[tri]\n",
    "    x_valid = train_ft.iloc[vai][selected_cols]\n",
    "    y_valid = target.iloc[vai]\n",
    "\n",
    "    # Stacking 모델 학습\n",
    "    stacking_model.fit(x_train, y_train)\n",
    "\n",
    "    # 학습된 모델을 복사하여 저장\n",
    "    models.append(copy.deepcopy(stacking_model))\n",
    "\n",
    "    # 검증 데이터 예측 및 점수 계산\n",
    "    pred = stacking_model.predict(x_valid)\n",
    "    score = f1_score(y_valid, pred, average='macro')\n",
    "    scores.append(score)\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "print(\"Mean F1 Macro Score (Stacking Ensemble):\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ensemble_pred = np.mean([model.predict_proba(test_ft[selected_cols]) for model in models], axis=0)\n",
    "threshold = 0.42\n",
    "submit[\"target\"] = (ensemble_pred[:, 1] >= threshold).astype(int)\n",
    "submit.to_csv(\"mlp_stacking_ensemble_sub.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Model F1 Macro Score: 0.7202856739266279\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import StackingClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# estimators = [\n",
    "#     ('mlp', MLPClassifier(**best_mlp_params, random_state=SEED)),\n",
    "#     ('rf', RandomForestClassifier(random_state=SEED)),\n",
    "# ]\n",
    "# stacking_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "# stacking_model.fit(x_train, y_train)\n",
    "\n",
    "# pred = stacking_model.predict(x_valid)\n",
    "# score = f1_score(y_valid, pred, average='macro')\n",
    "# print(\"Stacking Model F1 Macro Score:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#임계값 조절 submit\n",
    "threshold = 0.4\n",
    "sub_pred = stacking_model.predict_proba(test_ft[selected_cols])\n",
    "submit[\"target\"] = (sub_pred[:, 1] >= threshold).astype(int)\n",
    "submit.to_csv(\"mlp_stacking_sub.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #proba 파일\n",
    "\n",
    "# mlp_model = MLPClassifier(**best_mlp_params, random_state=SEED)\n",
    "# mlp_model.fit(train_ft[selected_cols], target)\n",
    "\n",
    "\n",
    "# pred = mlp_model.predict_proba(test_ft[selected_cols])[:, 1]\n",
    "\n",
    "# submit[\"target\"] = pred\n",
    "# submit.to_csv(\"mlp+optuna+logistic_fs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #submit 파일\n",
    "\n",
    "# sub_pred = mlp_model.predict_proba(test_ft[selected_cols])\n",
    "# submit[\"target\"] = sub_pred\n",
    "# submit.to_csv(\"mlp_optuna_logistic_fs_sub.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
